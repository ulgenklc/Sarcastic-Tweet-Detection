{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Twitter Sentiment Analysis\n\n**By Neuromatch Academy**\n\n__Content creators:__  Juan Manuel Rodriguez, Salomey Osei, Gonzalo Uribarri","metadata":{"id":"ca_6w0MZ2C76"}},{"cell_type":"markdown","source":"**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n\n<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>","metadata":{"id":"sKcIXMtu4IIj"}},{"cell_type":"markdown","source":"---\n# Step 1: Questions and goals\n\n* Can we infer emotion from a tweet text?\n* How words are distributed accross the dataset?\n* Are words related to one kind of emotion?","metadata":{"id":"qPDnA4tP4O8d"}},{"cell_type":"markdown","source":"---\n# Step 2: Literature review\n\n[Original Dataset Paper](https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf)\n\n[Papers with code](https://paperswithcode.com/dataset/imdb-movie-reviews)","metadata":{"id":"Yz2milFF4Q9a"}},{"cell_type":"markdown","source":"---\n# Step 3: Load and explore the dataset","metadata":{"id":"j3MSPcueSiSN"}},{"cell_type":"markdown","source":"##  Install dependencies\n","metadata":{"id":"TWEPUsg8pUyc"}},{"cell_type":"code","source":"# @title Install dependencies\n# !pip install pandas --quiet\n# !pip install torchtext --quiet","metadata":{"id":"vjyDH-sH6Rsa","tags":["hide-input"],"execution":{"iopub.status.busy":"2021-08-11T16:44:00.611941Z","iopub.execute_input":"2021-08-11T16:44:00.612249Z","iopub.status.idle":"2021-08-11T16:44:00.616082Z","shell.execute_reply.started":"2021-08-11T16:44:00.612182Z","shell.execute_reply":"2021-08-11T16:44:00.614970Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# We import some libraries to load the dataset\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport torchtext\nfrom torchtext.data import get_tokenizer\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport cudf","metadata":{"id":"-fZ9-oUPZCnN","execution":{"iopub.status.busy":"2021-08-11T17:02:06.733768Z","iopub.execute_input":"2021-08-11T17:02:06.734287Z","iopub.status.idle":"2021-08-11T17:02:09.653782Z","shell.execute_reply.started":"2021-08-11T17:02:06.734248Z","shell.execute_reply":"2021-08-11T17:02:09.652545Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"You can find the dataset we are going to use in [this website](http://help.sentiment140.com/for-students/).","metadata":{"id":"yImD1N69kBcK"}},{"cell_type":"code","source":"import requests, zipfile, io\nurl = 'http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip'\nr = requests.get(url)\nz = zipfile.ZipFile(io.BytesIO(r.content))\nz.extractall()","metadata":{"id":"1KCH2amLNcZH","execution":{"iopub.status.busy":"2021-08-11T16:44:02.797750Z","iopub.execute_input":"2021-08-11T16:44:02.798171Z","iopub.status.idle":"2021-08-11T16:44:10.779891Z","shell.execute_reply.started":"2021-08-11T16:44:02.798122Z","shell.execute_reply":"2021-08-11T16:44:10.778852Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# We load the dataset\nheader_list = [\"polarity\", \"id\", \"date\", \"query\", \"user\", \"text\"]\ndf = pd.read_csv('training.1600000.processed.noemoticon.csv',\n                 encoding = \"ISO-8859-1\", names=header_list)\n\n# Let's have a look at it\n# df.head()","metadata":{"id":"MKzdL9G-WEEx","outputId":"1346db8a-ac54-4b6e-ff76-63f5013e7a05","execution":{"iopub.status.busy":"2021-08-11T17:09:04.397832Z","iopub.execute_input":"2021-08-11T17:09:04.398178Z","iopub.status.idle":"2021-08-11T17:09:08.061161Z","shell.execute_reply.started":"2021-08-11T17:09:04.398149Z","shell.execute_reply":"2021-08-11T17:09:08.060257Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"For this project we will use only the text and the polarity of the tweet. Notice that polarity is 0 for negative tweets and 4 for positive tweet.","metadata":{"id":"JivOZOKwmQr5"}},{"cell_type":"markdown","source":"The first thing we have to do before working on the models is to familiarize ourselves with the dataset. This is called Exploratory Data Analisys (EDA).","metadata":{"id":"Le2EXidHSrsc"}},{"cell_type":"code","source":"######################################################################################\n#remove URLs\nimport re\npattern=r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))';\ndef rem_url(x):\n  match = re.findall(pattern, x)\n  for m in match:\n    url = m[0]\n    x = x.replace(url, '')\n  return x\n\ndf1['url_removed'] = df1['text'].apply(lambda x:rem_url(x))\n# df1.head()\nprint('URL removed')\n\n######################################################################################\n#remove twitter handles from text\nfrom nltk.tokenize import TweetTokenizer\ntknzr = TweetTokenizer(strip_handles=True)\ndf1['handle_removed'] = df1['url_removed'].apply(lambda x:tknzr.tokenize(x))\n# df1.head()\nprint('Twitter Handles removed')\n# detokenize the tweet again\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\ndf1['orig'] = df1['handle_removed'].apply(lambda x:TreebankWordDetokenizer().detokenize(x))\nprint('Tweets detokenized again')\n# df1.head()\n######################################################################################\n# remove punctuations\nimport string\nstring.punctuation\ndef remove_punctuation(text):\n    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n    return punctuationfree\ndf1['clean_msg']= df1['orig'].apply(lambda x:remove_punctuation(x))\n# df1.head()\nprint('Punctuation removed')\n######################################################################################\n# delete the unnecessary columns\ndel df1['url_removed']\ndel df1['handle_removed']\ndel df1['orig']\n# df1.head()\n######################################################################################\n# remove the upper case letters\ndf1['msg_lower'] = df1['clean_msg'].apply(lambda x: x.lower())\n# df1.head()\nprint('All set to lower case')\n######################################################################################\n# tokenize\nimport re\ndef tokenization(text):\n    tokens = re.split('W+',text)\n    return tokens\n#applying function to the column\n# df1['msg_tokenied']= df1['msg_lower'].apply(lambda x: tokenization(x))\nfrom nltk.tokenize import TweetTokenizer\ntknzr = TweetTokenizer(strip_handles=True)\ndf1['msg_tokenied'] = df1['msg_lower'].apply(lambda x:tknzr.tokenize(x))\n# df1.head()\nprint('Tweet tokenized again')\n######################################################################################\n# removing stop words\n#importing nlp library\nimport nltk\nnltk.download('stopwords')\n#Stop words present in the library\nstopwords = nltk.corpus.stopwords.words('english')\n# stopwords[0:10]\n\n#defining the function to remove stopwords from tokenized text\ndef remove_stopwords(text):\n    output= [i for i in text if i not in stopwords]\n    return output\n\n#applying the function\ndf1['no_stopwords']= df1['msg_tokenied'].apply(lambda x:remove_stopwords(x))\n# df1.head()\nprint('Stopwords removed')\n######################################################################################\n#importing the Stemming function from nltk library\nfrom nltk.stem.porter import PorterStemmer\n#defining the object for stemming\nporter_stemmer = PorterStemmer()\n#defining a function for stemming\ndef stemming(text):\n  stem_text = [porter_stemmer.stem(word) for word in text]\n  return stem_text\n\ndf1['msg_stemmed']=df1['no_stopwords'].apply(lambda x: stemming(x))\n# df1.head()\nprint('Tweets stemmed')\n######################################################################################\nfrom nltk.stem import WordNetLemmatizer\n#defining the object for Lemmatization\nwordnet_lemmatizer = WordNetLemmatizer()\n# nltk.download('wordnet')\n#defining the function for lemmatization\ndef lemmatizer(text):\n  lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n  return lemm_text\n\ndf1['msg_lemmatized']=df1['no_stopwords'].apply(lambda x:lemmatizer(x))\n# df1.head()\nprint('Tweets Lemmatized')\n######################################################################################\n\ndf2 = df1.copy()\n","metadata":{"id":"nWGfZbNBpc_v","execution":{"iopub.status.busy":"2021-08-11T17:10:29.078152Z","iopub.execute_input":"2021-08-11T17:10:29.078483Z","iopub.status.idle":"2021-08-11T17:10:29.262142Z","shell.execute_reply.started":"2021-08-11T17:10:29.078453Z","shell.execute_reply":"2021-08-11T17:10:29.260543Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-fbfa9ae33e0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url_removed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrem_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# df1.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'URL removed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'apply'"],"ename":"AttributeError","evalue":"'Series' object has no attribute 'apply'","output_type":"error"}]},{"cell_type":"code","source":"df2.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T17:00:06.347791Z","iopub.execute_input":"2021-08-11T17:00:06.348143Z","iopub.status.idle":"2021-08-11T17:00:06.377157Z","shell.execute_reply.started":"2021-08-11T17:00:06.348113Z","shell.execute_reply":"2021-08-11T17:00:06.376053Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   polarity          id                          date     query  \\\n0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n\n              user                                               text  \\\n0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n1    scotthamilton  is upset that he can't update his Facebook by ...   \n2         mattycus  @Kenichan I dived many times for the ball. Man...   \n3          ElleCTF    my whole body feels itchy and like its on fire    \n4           Karoli  @nationwideclass no, it's not behaving at all....   \n\n                                           clean_msg  \\\n0   Awwwthats a bummer  You shoulda got David Car...   \n1  is upset that he cant update his Facebook by t...   \n2  I dived many times for the ball  Managed to sa...   \n3     my whole body feels itchy and like its on fire   \n4  noits not behaving at all  im mad  why am i he...   \n\n                                           msg_lower  \\\n0   awwwthats a bummer  you shoulda got david car...   \n1  is upset that he cant update his facebook by t...   \n2  i dived many times for the ball  managed to sa...   \n3     my whole body feels itchy and like its on fire   \n4  noits not behaving at all  im mad  why am i he...   \n\n                                        msg_tokenied  \\\n0  [awwwthats, a, bummer, you, shoulda, got, davi...   \n1  [is, upset, that, he, cant, update, his, faceb...   \n2  [i, dived, many, times, for, the, ball, manage...   \n3  [my, whole, body, feels, itchy, and, like, its...   \n4  [noits, not, behaving, at, all, im, mad, why, ...   \n\n                                        no_stopwords  \\\n0  [awwwthats, bummer, shoulda, got, david, carr,...   \n1  [upset, cant, update, facebook, texting, itand...   \n2  [dived, many, times, ball, managed, save, 50, ...   \n3            [whole, body, feels, itchy, like, fire]   \n4  [noits, behaving, im, mad, herebecause, cant, ...   \n\n                                         msg_stemmed  \\\n0  [awwwthat, bummer, shoulda, got, david, carr, ...   \n1  [upset, cant, updat, facebook, text, itand, mi...   \n2  [dive, mani, time, ball, manag, save, 50, rest...   \n3             [whole, bodi, feel, itchi, like, fire]   \n4      [noit, behav, im, mad, herebecaus, cant, see]   \n\n                                      msg_lemmatized  \n0  [awwwthats, bummer, shoulda, got, david, carr,...  \n1  [upset, cant, update, facebook, texting, itand...  \n2  [dived, many, time, ball, managed, save, 50, r...  \n3             [whole, body, feel, itchy, like, fire]  \n4  [noits, behaving, im, mad, herebecause, cant, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>polarity</th>\n      <th>id</th>\n      <th>date</th>\n      <th>query</th>\n      <th>user</th>\n      <th>text</th>\n      <th>clean_msg</th>\n      <th>msg_lower</th>\n      <th>msg_tokenied</th>\n      <th>no_stopwords</th>\n      <th>msg_stemmed</th>\n      <th>msg_lemmatized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n      <td>Awwwthats a bummer  You shoulda got David Car...</td>\n      <td>awwwthats a bummer  you shoulda got david car...</td>\n      <td>[awwwthats, a, bummer, you, shoulda, got, davi...</td>\n      <td>[awwwthats, bummer, shoulda, got, david, carr,...</td>\n      <td>[awwwthat, bummer, shoulda, got, david, carr, ...</td>\n      <td>[awwwthats, bummer, shoulda, got, david, carr,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n      <td>is upset that he cant update his Facebook by t...</td>\n      <td>is upset that he cant update his facebook by t...</td>\n      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n      <td>[upset, cant, update, facebook, texting, itand...</td>\n      <td>[upset, cant, updat, facebook, text, itand, mi...</td>\n      <td>[upset, cant, update, facebook, texting, itand...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n      <td>I dived many times for the ball  Managed to sa...</td>\n      <td>i dived many times for the ball  managed to sa...</td>\n      <td>[i, dived, many, times, for, the, ball, manage...</td>\n      <td>[dived, many, times, ball, managed, save, 50, ...</td>\n      <td>[dive, mani, time, ball, manag, save, 50, rest...</td>\n      <td>[dived, many, time, ball, managed, save, 50, r...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n      <td>my whole body feels itchy and like its on fire</td>\n      <td>my whole body feels itchy and like its on fire</td>\n      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n      <td>[whole, body, feels, itchy, like, fire]</td>\n      <td>[whole, bodi, feel, itchi, like, fire]</td>\n      <td>[whole, body, feel, itchy, like, fire]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n      <td>noits not behaving at all  im mad  why am i he...</td>\n      <td>noits not behaving at all  im mad  why am i he...</td>\n      <td>[noits, not, behaving, at, all, im, mad, why, ...</td>\n      <td>[noits, behaving, im, mad, herebecause, cant, ...</td>\n      <td>[noit, behav, im, mad, herebecaus, cant, see]</td>\n      <td>[noits, behaving, im, mad, herebecause, cant, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# detokenize the tweet again\n# from nltk.tokenize.treebank import TreebankWordDetokenizer\n# df2['msg_lemmatized'] = df2['msg_lemmatized'].apply(lambda x:TreebankWordDetokenizer().detokenize(x))\n# df2['msg_stemmed'] = df2['msg_stemmed'].apply(lambda x:TreebankWordDetokenizer().detokenize(x))\ndel df2['msg_lower']\ndel df2['text']\ndel df2['clean_msg']\ndf2.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T17:00:06.378632Z","iopub.execute_input":"2021-08-11T17:00:06.379103Z","iopub.status.idle":"2021-08-11T17:00:06.480539Z","shell.execute_reply.started":"2021-08-11T17:00:06.379066Z","shell.execute_reply":"2021-08-11T17:00:06.479695Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   polarity          id                          date     query  \\\n0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n\n              user                                       msg_tokenied  \\\n0  _TheSpecialOne_  [awwwthats, a, bummer, you, shoulda, got, davi...   \n1    scotthamilton  [is, upset, that, he, cant, update, his, faceb...   \n2         mattycus  [i, dived, many, times, for, the, ball, manage...   \n3          ElleCTF  [my, whole, body, feels, itchy, and, like, its...   \n4           Karoli  [noits, not, behaving, at, all, im, mad, why, ...   \n\n                                        no_stopwords  \\\n0  [awwwthats, bummer, shoulda, got, david, carr,...   \n1  [upset, cant, update, facebook, texting, itand...   \n2  [dived, many, times, ball, managed, save, 50, ...   \n3            [whole, body, feels, itchy, like, fire]   \n4  [noits, behaving, im, mad, herebecause, cant, ...   \n\n                                         msg_stemmed  \\\n0  [awwwthat, bummer, shoulda, got, david, carr, ...   \n1  [upset, cant, updat, facebook, text, itand, mi...   \n2  [dive, mani, time, ball, manag, save, 50, rest...   \n3             [whole, bodi, feel, itchi, like, fire]   \n4      [noit, behav, im, mad, herebecaus, cant, see]   \n\n                                      msg_lemmatized  \n0  [awwwthats, bummer, shoulda, got, david, carr,...  \n1  [upset, cant, update, facebook, texting, itand...  \n2  [dived, many, time, ball, managed, save, 50, r...  \n3             [whole, body, feel, itchy, like, fire]  \n4  [noits, behaving, im, mad, herebecause, cant, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>polarity</th>\n      <th>id</th>\n      <th>date</th>\n      <th>query</th>\n      <th>user</th>\n      <th>msg_tokenied</th>\n      <th>no_stopwords</th>\n      <th>msg_stemmed</th>\n      <th>msg_lemmatized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>[awwwthats, a, bummer, you, shoulda, got, davi...</td>\n      <td>[awwwthats, bummer, shoulda, got, david, carr,...</td>\n      <td>[awwwthat, bummer, shoulda, got, david, carr, ...</td>\n      <td>[awwwthats, bummer, shoulda, got, david, carr,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n      <td>[upset, cant, update, facebook, texting, itand...</td>\n      <td>[upset, cant, updat, facebook, text, itand, mi...</td>\n      <td>[upset, cant, update, facebook, texting, itand...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>[i, dived, many, times, for, the, ball, manage...</td>\n      <td>[dived, many, times, ball, managed, save, 50, ...</td>\n      <td>[dive, mani, time, ball, manag, save, 50, rest...</td>\n      <td>[dived, many, time, ball, managed, save, 50, r...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n      <td>[whole, body, feels, itchy, like, fire]</td>\n      <td>[whole, bodi, feel, itchi, like, fire]</td>\n      <td>[whole, body, feel, itchy, like, fire]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>[noits, not, behaving, at, all, im, mad, why, ...</td>\n      <td>[noits, behaving, im, mad, herebecause, cant, ...</td>\n      <td>[noit, behav, im, mad, herebecaus, cant, see]</td>\n      <td>[noits, behaving, im, mad, herebecause, cant, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df2.to_csv('preprocessed_training_data_v2.csv')\n","metadata":{"execution":{"iopub.status.busy":"2021-08-11T17:00:06.482870Z","iopub.execute_input":"2021-08-11T17:00:06.483269Z","iopub.status.idle":"2021-08-11T17:00:39.774697Z","shell.execute_reply.started":"2021-08-11T17:00:06.483229Z","shell.execute_reply":"2021-08-11T17:00:39.773821Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# If the data set is already loaded in the directory start from here.\n# df3 = pd.read_csv(\"../input/twitter-training-preprocessed/preprocessed_training_data.csv\", index_col = 0)\n# df3.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T17:00:39.776154Z","iopub.execute_input":"2021-08-11T17:00:39.776553Z","iopub.status.idle":"2021-08-11T17:00:39.782379Z","shell.execute_reply.started":"2021-08-11T17:00:39.776513Z","shell.execute_reply":"2021-08-11T17:00:39.780546Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# let's play with the lemmatized text to see how it's accuracy fairs\nX = df2.msg_stemmed.values\n# print(X.shape)\n# Changes values from [0,4] to [0,1]\ny = (df2.polarity.values > 1).astype(int)\n\n\n# Split the data into train and test\nx_train_text, x_test_text, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n\n# print out the sample data set\nfor s, l in zip(x_train_text[:5], y_train[:5]):\n  print('{}: {}'.format(l, s))\n\n# # tokenize the text for furhter processing\n# tokenizer = get_tokenizer(\"basic_english\")\n\nx_train_token = x_train_text #[tokenizer(s) for s in tqdm(x_train_text)]\nx_test_token = x_test_text #[tokenizer(s) for s in tqdm(x_test_text)]\n\n# create a word counter\nwords = Counter()\nfor s in x_train_token:\n  for w in s:\n    words[w] += 1\n\n#sort the words\nsorted_words = list(words.keys())\nsorted_words.sort(key=lambda w: words[w], reverse=True)\nprint(f\"Number of different Tokens in our Dataset: {len(sorted_words)}\")\nprint(sorted_words[:10])\n\ncount_occurences = sum(words.values())\n\naccumulated = 0\ncounter = 0\n\nwhile accumulated < count_occurences * 0.8:\n  accumulated += words[sorted_words[counter]]\n  counter += 1\n\nprint(f\"The {counter * 100 / len(words)}% most common words \"\n      f\"account for the {accumulated * 100 / count_occurences}% of the occurrences\")\n\n# plt.bar(range(50), [words[w] for w in sorted_words[:50]])\n# plt.show()\n\n# #print the first 10 most common words\n# for w in sorted_words[:10]:\n#   print(w, words[w])","metadata":{"execution":{"iopub.status.busy":"2021-08-11T17:00:39.783610Z","iopub.execute_input":"2021-08-11T17:00:39.784005Z","iopub.status.idle":"2021-08-11T17:00:46.216740Z","shell.execute_reply.started":"2021-08-11T17:00:39.783968Z","shell.execute_reply":"2021-08-11T17:00:46.215839Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"1: ['lol', 'get', 'idea', 'far', 'advanceit', 'even', 'june', 'yetw', 'need', 'third', 'knitter', 'summer', 'group']\n0: ['worst', 'headach', 'ever']\n0: ['sad', 'wont', 'see', 'youi', 'miss', 'alreadi', 'yeahthat', 'perfect', 'come', 'back', '18th']\n1: ['doesnt', 'know', 'spell', 'conk']\n0: ['stand', 'one', 'know', 'us', 'wont', 'get', 'use', 'wont', 'get', 'use', 'gone', 'miss', 'home', 'everyon']\nNumber of different Tokens in our Dataset: 610601\n['im', 'go', 'get', 'day', 'good', 'like', 'work', 'love', 'dont', 'got']\nThe 0.6280697214711407% most common words account for the 80.00203885772501% of the occurrences\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Recurrent Neural Network with Pytorch","metadata":{"id":"zVZ2PBxbvKQe"}},{"cell_type":"code","source":"def set_device():\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n          \"if possible, in the menu under `Runtime` -> \"\n          \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device\n\n# Set the device (check if gpu is available)\ndevice = set_device()","metadata":{"id":"b71EOPSW3ZGX","execution":{"iopub.status.busy":"2021-08-11T14:53:29.824845Z","iopub.execute_input":"2021-08-11T14:53:29.825157Z","iopub.status.idle":"2021-08-11T14:53:29.894114Z","shell.execute_reply.started":"2021-08-11T14:53:29.82513Z","shell.execute_reply":"2021-08-11T14:53:29.893069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we will create a Dictionary (`word_to_idx`). This dictionary will map each Token (usually words) to an index (an integer number). We want to limit our dictionary to a certain number of tokens (`num_words_dict`), so we will include in our ditionary those with more occurrences.","metadata":{"id":"o-PDXYBeHkP8"}},{"cell_type":"code","source":"#Let us load the Glove embedded vectors\n!wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n!unzip glove.6B.zip\n# !wget https://nlp.stanford.edu/data/glove.twitter.27B.zip\n# !unzip glove.twitter.27B.zip","metadata":{"execution":{"iopub.status.busy":"2021-08-10T17:15:30.842136Z","iopub.execute_input":"2021-08-10T17:15:30.842456Z","iopub.status.idle":"2021-08-10T17:21:07.199352Z","shell.execute_reply.started":"2021-08-10T17:15:30.842429Z","shell.execute_reply":"2021-08-10T17:21:07.198429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# glove = pd.read_csv('glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n# glove_embedding = {key: val.values for key, val in glove.T.items()}\n\nglove = pd.read_csv('../input/glovetwitter27b100dtxt/glove.twitter.27B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)\nglove_embedding = {key: val.values for key, val in glove.T.items()}\n","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:54:53.510547Z","iopub.execute_input":"2021-08-11T14:54:53.510922Z","iopub.status.idle":"2021-08-11T14:55:58.931576Z","shell.execute_reply.started":"2021-08-11T14:54:53.510889Z","shell.execute_reply":"2021-08-11T14:55:58.930661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's select only the most used.\nnum_words_dict = 30000\n# We reserve two numbers for special tokens.\nmost_used_words = sorted_words[:num_words_dict-2]\n\n\n#We will add two extra Tokens to the dictionary, one for words outside the dictionary (`'UNK'`) and one for padding the sequences (`'PAD'`).\n\n# dictionary to go from words to idx \nword_to_idx = {}\n# dictionary to go from idx to words (just in case) \nidx_to_word = {}\n\n\n# We include the special tokens first\nPAD_token = 0   \nUNK_token = 1\n\nword_to_idx['PAD'] = PAD_token\nword_to_idx['UNK'] = UNK_token\n\nidx_to_word[PAD_token] = 'PAD'\nidx_to_word[UNK_token] = 'UNK'\n\n# We popullate our dictionaries with the most used words\nfor num,word in enumerate(most_used_words):\n  word_to_idx[word] = num + 2\n  idx_to_word[num+2] = word\n    \n    \ndef create_embedding_matrix(word_index,embedding_dict,dimension):\n  embedding_matrix=np.zeros((len(word_index)+1,dimension))\n \n  for word,index in word_index.items():\n    if word in embedding_dict:\n      embedding_matrix[index]=embedding_dict[word]\n  return embedding_matrix\n\n\n# My word_to_idx is the same as their word_index\nembedding_matrix=create_embedding_matrix(word_to_idx,embedding_dict=glove_embedding,dimension=100)\n\n\n#Our goal now is to transform each tweet from a sequence of tokens to a sequence of indexes. \n# These sequences of indexes will be the input to our pytorch model.\n\n# A function to convert list of tokens to list of indexes\ndef tokens_to_idx(sentences_tokens,word_to_idx):\n  sentences_idx = []\n  for sent in sentences_tokens:\n    sent_idx = []\n    for word in sent:\n      if word in word_to_idx:\n        sent_idx.append(word_to_idx[word])\n      else:\n        sent_idx.append(word_to_idx['UNK'])\n    sentences_idx.append(sent_idx)\n  return sentences_idx\n\nx_train_idx = tokens_to_idx(x_train_token,word_to_idx)\nx_test_idx = tokens_to_idx(x_test_token,word_to_idx)\n\n\n\n# We need all the sequences to have the same length. \n# To select an adequate sequence length, let's explore some statistics about the length of the tweets:\ntweet_lens = np.asarray([len(sentence) for sentence in x_train_idx])\nprint('Max tweet word length: ',tweet_lens.max())\nprint('Mean tweet word length: ',np.median(tweet_lens))\nprint('99% percent under: ',np.quantile(tweet_lens,0.99))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:56:33.397643Z","iopub.execute_input":"2021-08-11T14:56:33.398022Z","iopub.status.idle":"2021-08-11T14:56:40.399175Z","shell.execute_reply.started":"2021-08-11T14:56:33.39799Z","shell.execute_reply":"2021-08-11T14:56:40.398206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# most_used_words[45]\n# embedding_matrix[45,:]","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:10:12.677242Z","iopub.execute_input":"2021-08-11T15:10:12.67758Z","iopub.status.idle":"2021-08-11T15:10:12.682632Z","shell.execute_reply.started":"2021-08-11T15:10:12.677549Z","shell.execute_reply":"2021-08-11T15:10:12.681789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We cut the sequences which are larger than our chosen maximum length (`max_lenght`) and fill with zeros the ones that are shorter.\n# We choose the max length\nmax_length = 20\n\n# A function to make all the sequence have the same lenght\n# Note that the output is a Numpy matrix\ndef padding(sentences, seq_len):\n features = np.zeros((len(sentences), seq_len),dtype=int)\n for ii, tweet in enumerate(sentences):\n   len_tweet = len(tweet) \n   if len_tweet != 0:\n     if len_tweet <= seq_len:\n       # If its shorter, we fill with zeros (the padding Token index)\n       features[ii, -len(tweet):] = np.array(tweet)[:seq_len]\n     if len_tweet > seq_len:\n       # If its larger, we take the last 'seq_len' indexes\n       features[ii, :] = np.array(tweet)[-seq_len:]\n return features\n\n\n# We convert our list of tokens into a numpy matrix\n# where all instances have the same lenght\nx_train_pad = padding(x_train_idx,max_length)\nx_test_pad = padding(x_test_idx,max_length)\n\n# We convert our target list a numpy matrix\ny_train_np = np.asarray(y_train)\ny_test_np = np.asarray(y_test)\n\n\n# Now, let's convert the data to pytorch format.\n\n# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train_np))\nvalid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test_np))\n\n# Batch size (this is an important hyperparameter)\nbatch_size = 100\n\n# dataloaders\n# make sure to SHUFFLE your data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last = True)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last = True)\n\n\n# Each batch of data in our traning proccess will have the folllowing format:\n# Obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\n\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint('Sample input: \\n', sample_y)","metadata":{"id":"n4lowj5J3bbU","execution":{"iopub.status.busy":"2021-08-11T15:14:46.016747Z","iopub.execute_input":"2021-08-11T15:14:46.017089Z","iopub.status.idle":"2021-08-11T15:14:54.479096Z","shell.execute_reply.started":"2021-08-11T15:14:46.01706Z","shell.execute_reply":"2021-08-11T15:14:54.477548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will define the `SentimentRNN` class. Most of the model's class will be familiar to you, but there are two important layers we would like you to pay attention to:\n\n*   Embedding Layer\n> This layer is like a linear layer, but it makes it posible to use a sequence of inedexes as inputs (instead of a sequence of one-hot-encoded vectors). During training, the Embedding layer learns a linear transformation from the space of words (a vector space of dimension `num_words_dict`) into the a new, smaller, vector space of dimension `embedding_dim`. We suggest you to read this [thread](https://discuss.pytorch.org/t/how-does-nn-embedding-work/88518/3) and the [pytorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) if you want to learn more about this particular kind of layers.\n\n\n*   LSTM layer\n> This is one of the most used class of Recurrent Neural Networks. In Pytorch we can add several stacked layers in just one line of code. In our case, the number of layers added are decided with the parameter `no_layers`. If you want to learn more about LSTMs we strongly recommend you this [Colahs thread](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) about them.\n\n\n\n\n\n","metadata":{"id":"OlgELk0iZJRa"}},{"cell_type":"code","source":"class SentimentRNN(nn.Module):\n  def __init__(self,no_layers,hidden_dim,embedding_matrix,drop_prob=0.1):\n    super(SentimentRNN,self).__init__()\n\n    self.output_dim = output_dim\n    self.hidden_dim = hidden_dim\n    self.no_layers = no_layers\n    self.vocab_size = vocab_size\n    self.drop_prob = drop_prob\n    self.vocab_size=embedding_matrix.shape[0]\n    self.embedding_dim=embedding_matrix.shape[1]\n    \n    # Let us set up the embedding layer\n    self.embedding=nn.Embedding(self.vocab_size,self.embedding_dim)\n    self.embedding.weight=nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n    self.embedding.weight.requires_grad=True\n\n    # Embedding Layer\n#     self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n    # LSTM Layers\n    self.lstm = nn.LSTM(input_size=self.embedding_dim,hidden_size=self.hidden_dim,\n                        num_layers=no_layers, bidirectional=False, batch_first=True, \n                        dropout=self.drop_prob)\n#     print(self.lstm.shape)\n    # Dropout layer\n    self.dropout = nn.Dropout(drop_prob)\n\n    # Linear and Sigmoid layer\n    self.fc = nn.Linear(self.hidden_dim, output_dim)\n    self.sig = nn.Sigmoid()\n      \n  def forward(self,x,hidden):\n    batch_size = x.size(0)\n    embeds = self.embedding(x)\n    #Shape: [batch_size x max_length x embedding_dim]\n    \n    # LSTM out\n    lstm_out, hidden = self.lstm(embeds, hidden)\n    # Shape: [batch_size x max_length x hidden_dim]\n    \n    # Select the activation of the last Hidden Layer\n    lstm_out = lstm_out[:,-1,:].contiguous()\n    # Shape: [batch_size x hidden_dim]\n\n    ## You can instead average the activations across all the times\n    # lstm_out = torch.mean(lstm_out, 1).contiguous()\n\n    # Dropout and Fully connected layer\n    out = self.dropout(lstm_out)\n    out = self.fc(out)\n\n    # Sigmoid function\n    sig_out = self.sig(out)\n\n    # return last sigmoid output and hidden state\n    return sig_out, hidden\n\n  def init_hidden(self, batch_size):\n    ''' Initializes hidden state '''\n    # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n    # initialized to zero, for hidden state and cell state of LSTM\n    h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n    \n    c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n    \n    hidden = (h0,c0)\n    return hidden\n\n#######################################################################################\n# Parameters of our network\n\n# Size of our vocabulary\nvocab_size = num_words_dict\n\n# Embedding dimension\n# embedding_dim = 00\n\n# Number of stacked LSTM layers\nno_layers = 2\n\n# Dimension of the hidden layer in LSTMs\nhidden_dim = 64\n\n# Dropout parameter for regularization\noutput_dim = 1\n\n# Dropout parameter for regularization\ndrop_prob = 0.25\n\n# Let's define our model\nmodel = SentimentRNN(no_layers, hidden_dim,embedding_matrix, drop_prob=drop_prob)\n# Moving to gpu\nmodel.to(device)\nprint(model)\n\n# How many trainable parameters does our model have?\nmodel_parameters = filter(lambda p: p.requires_grad, model.parameters())\nparams = sum([np.prod(p.size()) for p in model_parameters])\nprint('Total Number of parameters: ',params)\n\n# loss and optimization functions\nlr = 0.005\n\n# Binary crossentropy is a good loss function for a binary classification problem\ncriterion = nn.BCELoss()\n\n# We choose an Adam optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# function to predict accuracy\ndef acc(pred,label):\n  pred = torch.round(pred.squeeze())\n  return torch.sum(pred == label.squeeze()).item()\n#######################################################################################\n#lets run the model for one epoch to test its ability to run.\n# Number of training Epochs\nepochs = 2\n\n# Maximum absolute value accepted for the gradeint\nclip = 5\n\n# Initial Loss value (assumed big)\nvalid_loss_min = np.Inf\n\n# Lists to follow the evolution of the loss and accuracy\nepoch_tr_loss,epoch_vl_loss = [],[]\nepoch_tr_acc,epoch_vl_acc = [],[]\n\n# Train for a number of Epochs\nfor epoch in range(epochs):\n  train_losses = []\n  train_acc = 0.0\n  model.train()\n  \n  for inputs, labels in train_loader:\n\n    # Initialize hidden state \n    h = model.init_hidden(batch_size)\n    # Creating new variables for the hidden state\n    h = tuple([each.data.to(device) for each in h])\n\n    # Move batch inputs and labels to gpu\n    inputs, labels = inputs.to(device), labels.to(device)   \n\n    # Set gradient to zero\n    model.zero_grad()\n\n    # Compute model output\n    output,h = model(inputs,h)\n\n    # Calculate the loss and perform backprop\n    loss = criterion(output.squeeze(), labels.float())\n    loss.backward()\n    train_losses.append(loss.item())\n\n    # calculating accuracy\n    accuracy = acc(output,labels)\n    train_acc += accuracy\n\n    #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n    nn.utils.clip_grad_norm_(model.parameters(), clip)\n    optimizer.step()\n\n  \n  # Evaluate on the validation set for this epoch \n  val_losses = []\n  val_acc = 0.0\n  model.eval()\n  for inputs, labels in valid_loader:\n\n    # Initialize hidden state \n    val_h = model.init_hidden(batch_size)\n    val_h = tuple([each.data.to(device) for each in val_h])\n\n    # Move batch inputs and labels to gpu\n    inputs, labels = inputs.to(device), labels.to(device)\n\n    # Compute model output\n    output, val_h = model(inputs, val_h)\n\n    # Compute Loss\n    val_loss = criterion(output.squeeze(), labels.float())\n\n    val_losses.append(val_loss.item())\n\n    accuracy = acc(output,labels)\n    val_acc += accuracy\n          \n  epoch_train_loss = np.mean(train_losses)\n  epoch_val_loss = np.mean(val_losses)\n  epoch_train_acc = train_acc/len(train_loader.dataset)\n  epoch_val_acc = val_acc/len(valid_loader.dataset)\n  epoch_tr_loss.append(epoch_train_loss)\n  epoch_vl_loss.append(epoch_val_loss)\n  epoch_tr_acc.append(epoch_train_acc)\n  epoch_vl_acc.append(epoch_val_acc)\n  print(f'Epoch {epoch+1}') \n  print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n  print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n  if epoch_val_loss <= valid_loss_min:\n    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n    # torch.save(model.state_dict(), '../working/state_dict.pt')\n    valid_loss_min = epoch_val_loss\n  print(25*'==')\n\n#######################################################################################\n# plot the results from the training and validation accuracies\nfig = plt.figure(figsize = (20, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epoch_tr_acc, label='Train Acc')\nplt.plot(epoch_vl_acc, label='Validation Acc')\n# plt.ylim([70, 80])\nplt.title(\"Accuracy\")\nplt.legend()\nplt.grid()\n\nplt.subplot(1, 2, 2)\nplt.plot(epoch_tr_loss, label='Train loss')\nplt.plot(epoch_vl_loss, label='Validation loss')\nplt.title(\"Loss\")\nplt.legend()\nplt.grid()\n\nplt.show()","metadata":{"id":"k1T6TCkNv3vh","execution":{"iopub.status.busy":"2021-08-11T15:33:41.14799Z","iopub.execute_input":"2021-08-11T15:33:41.148316Z","iopub.status.idle":"2021-08-11T15:37:06.909834Z","shell.execute_reply.started":"2021-08-11T15:33:41.148284Z","shell.execute_reply":"2021-08-11T15:37:06.909043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentimentRNN(nn.Module):\n  def __init__(self,no_layers,hidden_dim,embedding_matrix,drop_prob = 0.01):\n    super(SentimentRNN,self).__init__()\n\n    self.output_dim = output_dim\n    self.hidden_dim = hidden_dim\n    self.no_layers = no_layers\n    self.vocab_size = vocab_size\n    self.drop_prob = drop_prob\n    self.vocab_size=embedding_matrix.shape[0]\n    self.embedding_dim=embedding_matrix.shape[1]\n    \n    # Let us set up the embedding layer\n    self.embedding=nn.Embedding(self.vocab_size,self.embedding_dim)\n    self.embedding.weight=nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n    self.embedding.weight.requires_grad=False\n\n    # LSTM Layers\n    self.gru = nn.GRU(input_size= self.embedding_dim,hidden_size=self.hidden_dim,\n                        num_layers=no_layers, bidirectional=False, batch_first=True, \n                        dropout=self.drop_prob)\n\n    # Dropout layer\n    self.dropout = nn.Dropout(drop_prob)\n\n    # Linear and Sigmoid layer\n    self.fc = nn.Linear(self.hidden_dim, output_dim)\n    self.sig = nn.Sigmoid()\n      \n  def forward(self,x,hidden):\n    batch_size = x.size(0)\n    self.h = self.init_hidden(batch_size)\n    \n    # Embedding out\n    embeds = self.embedding(x)\n    #Shape: [batch_size x max_length x embedding_dim]\n\n    # GRU out\n    gru_out, self.h = self.gru(embeds, self.h)\n    # lstm_out, hidden = self.lstm(embeds, hidden)\n    # Shape: [batch_size x max_length x hidden_dim]\n\n    # Select the activation of the last Hidden Layer\n    gru_out = gru_out[:,-1,:].contiguous()\n    # Shape: [batch_size x hidden_dim]\n\n    ## You can instead average the activations across all the times\n    # lstm_out = torch.mean(lstm_out, 1).contiguous()\n\n    # Dropout and Fully connected layer\n    out = self.dropout(gru_out)\n    out = self.fc(out)\n\n    # Sigmoid function\n    sig_out = self.sig(out)\n\n    # return last sigmoid output and hidden state\n    return sig_out #, hidden\n  def init_hidden(self, batch_size):\n    hidden = (torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device))\n    return hidden\n\n  # def init_hidden(self, batch_size):\n  #   ''' Initializes hidden state '''\n  #   # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n  #   # initialized to zero, for hidden state and cell state of LSTM\n  #   h0 = torch.zeros((self.no_layers, batch_size,self.hidden_dim)).to(device)\n  #   c0 = torch.zeros((self.no_layers, batch_size,self.hidden_dim)).to(device)\n  #   hidden = (h0,c0)\n  #   return hidden\n\n#######################################################################################\n# Parameters of our network\n\n# Size of our vocabulary\nvocab_size = num_words_dict\n\n# Embedding dimension\nembedding_dim = 32\n\n# Number of stacked LSTM layers\nno_layers = 2\n\n# Dimension of the hidden layer in LSTMs\nhidden_dim = 64\n\n# Dropout parameter for regularization\noutput_dim = 1\n\n# Dropout parameter for regularization\ndrop_prob = 0.25\n\n# Let's define our model\nmodel = SentimentRNN(no_layers, hidden_dim,\n                     embedding_matrix, drop_prob=drop_prob)\n# Moving to gpu\nmodel.to(device)\nprint(model)\n\n# How many trainable parameters does our model have?\nmodel_parameters = filter(lambda p: p.requires_grad, model.parameters())\nparams = sum([np.prod(p.size()) for p in model_parameters])\nprint('Total Number of parameters: ',params)\n\n# loss and optimization functions\nlr = 0.001\n\n# Binary crossentropy is a good loss function for a binary classification problem\ncriterion = nn.BCELoss()\n\n# We choose an Adam optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# function to predict accuracy\ndef acc(pred,label):\n  pred = torch.round(pred.squeeze())\n  return torch.sum(pred == label.squeeze()).item()\n\n# Number of training Epochs\nepochs = 1\n\n# Maximum absolute value accepted for the gradeint\nclip = 5\n\n# Initial Loss value (assumed big)\nvalid_loss_min = np.Inf\n\n# Lists to follow the evolution of the loss and accuracy\nepoch_tr_loss,epoch_vl_loss = [],[]\nepoch_tr_acc,epoch_vl_acc = [],[]\n\n# Train for a number of Epochs\nfor epoch in range(epochs):\n  train_losses = []\n  train_acc = 0.0\n  model.train()\n  \n  for inputs, labels in train_loader:\n\n    # Initialize hidden state \n    # h = model.init_hidden(batch_size)\n    # Creating new variables for the hidden state\n    # h = tuple([each.data.to(device) for each in h])\n    # print(h[1].shape)\n    # Move batch inputs and labels to gpu\n    inputs, labels = inputs.to(device), labels.to(device)   \n\n    # Set gradient to zero\n    model.zero_grad()\n\n    # Compute model output\n    output = model(inputs,h)\n\n    # Calculate the loss and perform backprop\n    loss = criterion(output.squeeze(), labels.float())\n    loss.backward()\n    train_losses.append(loss.item())\n\n    # calculating accuracy\n    accuracy = acc(output,labels)\n    train_acc += accuracy\n\n    #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n    nn.utils.clip_grad_norm_(model.parameters(), clip)\n    optimizer.step()\n\n  \n  # Evaluate on the validation set for this epoch \n  val_losses = []\n  val_acc = 0.0\n  model.eval()\n  for inputs, labels in valid_loader:\n\n    # Initialize hidden state \n    # val_h = model.init_hidden(batch_size)\n    # val_h = tuple([each.data.to(device) for each in val_h])\n\n    # Move batch inputs and labels to gpu\n    inputs, labels = inputs.to(device), labels.to(device)\n\n    # Compute model output\n    output = model(inputs, val_h)\n\n    # Compute Loss\n    val_loss = criterion(output.squeeze(), labels.float())\n\n    val_losses.append(val_loss.item())\n\n    accuracy = acc(output,labels)\n    val_acc += accuracy\n          \n  epoch_train_loss = np.mean(train_losses)\n  epoch_val_loss = np.mean(val_losses)\n  epoch_train_acc = train_acc/len(train_loader.dataset)\n  epoch_val_acc = val_acc/len(valid_loader.dataset)\n  epoch_tr_loss.append(epoch_train_loss)\n  epoch_vl_loss.append(epoch_val_loss)\n  epoch_tr_acc.append(epoch_train_acc)\n  epoch_vl_acc.append(epoch_val_acc)\n  print(f'Epoch {epoch+1}') \n  print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n  print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n  if epoch_val_loss <= valid_loss_min:\n    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n    # torch.save(model.state_dict(), '../working/state_dict.pt')\n    valid_loss_min = epoch_val_loss\n  print(25*'==')\n\nfig = plt.figure(figsize = (20, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epoch_tr_acc, label='Train Acc')\nplt.plot(epoch_vl_acc, label='Validation Acc')\nplt.title(\"Accuracy\")\nplt.legend()\nplt.grid()\n\nplt.subplot(1, 2, 2)\nplt.plot(epoch_tr_loss, label='Train loss')\nplt.plot(epoch_vl_loss, label='Validation loss')\nplt.title(\"Loss\")\nplt.legend()\nplt.grid()\n\nplt.show()","metadata":{"id":"HjHgDrNs3n1R","outputId":"7fab01d5-1676-4f5b-9e30-b45181d62dff","execution":{"iopub.status.busy":"2021-08-11T16:04:05.823996Z","iopub.execute_input":"2021-08-11T16:04:05.824382Z","iopub.status.idle":"2021-08-11T16:05:40.483226Z","shell.execute_reply.started":"2021-08-11T16:04:05.824347Z","shell.execute_reply":"2021-08-11T16:05:40.482478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentimentRNN(nn.Module):\n  def __init__(self,no_layers,hidden_dim,embedding_matrix,drop_prob=0.1):\n    super(SentimentRNN,self).__init__()\n\n    self.output_dim = output_dim\n    self.hidden_dim = hidden_dim\n    self.no_layers = no_layers\n    self.no_layers_bi = no_layers * 2\n    self.vocab_size = vocab_size\n    self.drop_prob = drop_prob\n    self.vocab_size=embedding_matrix.shape[0]\n    self.embedding_dim=embedding_matrix.shape[1]\n    \n    # Let us set up the embedding layer\n    self.embedding=nn.Embedding(self.vocab_size,self.embedding_dim)\n    self.embedding.weight=nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n    self.embedding.weight.requires_grad=True\n\n    # Embedding Layer\n#     self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n    # LSTM Layers\n    self.lstm = nn.LSTM(input_size=self.embedding_dim,hidden_size=self.hidden_dim,\n                        num_layers=no_layers, bidirectional=True, batch_first=True, \n                        dropout=self.drop_prob)\n#     print(self.lstm.shape)\n    # Dropout layer\n    self.dropout = nn.Dropout(drop_prob)\n\n    # Linear and Sigmoid layer\n    self.fc = nn.Linear(self.hidden_dim * 2, output_dim)\n    self.sig = nn.Sigmoid()\n      \n  def forward(self,x,hidden):\n    batch_size = x.size(0)\n    embeds = self.embedding(x)\n    #Shape: [batch_size x max_length x embedding_dim]\n    \n    # LSTM out\n    lstm_out, hidden = self.lstm(embeds, hidden)\n    # Shape: [batch_size x max_length x hidden_dim]\n    \n    # Select the activation of the last Hidden Layer\n    lstm_out = lstm_out[:,-1,:].contiguous()\n    # Shape: [batch_size x hidden_dim]\n\n    ## You can instead average the activations across all the times\n    # lstm_out = torch.mean(lstm_out, 1).contiguous()\n\n    # Dropout and Fully connected layer\n    out = self.dropout(lstm_out)\n    out = self.fc(out)\n\n    # Sigmoid function\n    sig_out = self.sig(out)\n\n    # return last sigmoid output and hidden state\n    return sig_out, hidden\n\n  def init_hidden(self, batch_size):\n    ''' Initializes hidden state '''\n    # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n    # initialized to zero, for hidden state and cell state of LSTM\n    h0 = torch.zeros((self.no_layers * 2,batch_size, self.hidden_dim)).to(device)\n    c0 = torch.zeros((self.no_layers * 2,batch_size, self.hidden_dim)).to(device)\n    \n    hidden = (h0,c0)\n    return hidden\n\n#######################################################################################\n# Parameters of our network\n\n# Size of our vocabulary\nvocab_size = num_words_dict\n\n# Embedding dimension\n# embedding_dim = 00\n\n# Number of stacked LSTM layers\nno_layers = 2\n\n# Dimension of the hidden layer in LSTMs\nhidden_dim = 64\n\n# Dropout parameter for regularization\noutput_dim = 1\n\n# Dropout parameter for regularization\ndrop_prob = 0.25\n\n# Let's define our model\nmodel = SentimentRNN(no_layers, hidden_dim,embedding_matrix, drop_prob=drop_prob)\n# Moving to gpu\nmodel.to(device)\nprint(model)\n\n# How many trainable parameters does our model have?\nmodel_parameters = filter(lambda p: p.requires_grad, model.parameters())\nparams = sum([np.prod(p.size()) for p in model_parameters])\nprint('Total Number of parameters: ',params)\n\n# loss and optimization functions\nlr = 0.005\n\n# Binary crossentropy is a good loss function for a binary classification problem\ncriterion = nn.BCELoss()\n\n# We choose an Adam optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# function to predict accuracy\ndef acc(pred,label):\n  pred = torch.round(pred.squeeze())\n  return torch.sum(pred == label.squeeze()).item()\n#######################################################################################\n#lets run the model for one epoch to test its ability to run.\n# Number of training Epochs\nepochs = 2\n\n# Maximum absolute value accepted for the gradeint\nclip = 5\n\n# Initial Loss value (assumed big)\nvalid_loss_min = np.Inf\n\n# Lists to follow the evolution of the loss and accuracy\nepoch_tr_loss,epoch_vl_loss = [],[]\nepoch_tr_acc,epoch_vl_acc = [],[]\n\n# Train for a number of Epochs\nfor epoch in range(epochs):\n  train_losses = []\n  train_acc = 0.0\n  model.train()\n  \n  for inputs, labels in train_loader:\n\n    # Initialize hidden state \n    h = model.init_hidden(batch_size)\n    # Creating new variables for the hidden state\n    h = tuple([each.data.to(device) for each in h])\n\n    # Move batch inputs and labels to gpu\n    inputs, labels = inputs.to(device), labels.to(device)   \n\n    # Set gradient to zero\n    model.zero_grad()\n\n    # Compute model output\n    output,h = model(inputs,h)\n\n    # Calculate the loss and perform backprop\n    loss = criterion(output.squeeze(), labels.float())\n    loss.backward()\n    train_losses.append(loss.item())\n\n    # calculating accuracy\n    accuracy = acc(output,labels)\n    train_acc += accuracy\n\n    #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n    nn.utils.clip_grad_norm_(model.parameters(), clip)\n    optimizer.step()\n\n  \n  # Evaluate on the validation set for this epoch \n  val_losses = []\n  val_acc = 0.0\n  model.eval()\n  for inputs, labels in valid_loader:\n\n    # Initialize hidden state \n    val_h = model.init_hidden(batch_size)\n    val_h = tuple([each.data.to(device) for each in val_h])\n\n    # Move batch inputs and labels to gpu\n    inputs, labels = inputs.to(device), labels.to(device)\n\n    # Compute model output\n    output, val_h = model(inputs, val_h)\n\n    # Compute Loss\n    val_loss = criterion(output.squeeze(), labels.float())\n\n    val_losses.append(val_loss.item())\n\n    accuracy = acc(output,labels)\n    val_acc += accuracy\n          \n  epoch_train_loss = np.mean(train_losses)\n  epoch_val_loss = np.mean(val_losses)\n  epoch_train_acc = train_acc/len(train_loader.dataset)\n  epoch_val_acc = val_acc/len(valid_loader.dataset)\n  epoch_tr_loss.append(epoch_train_loss)\n  epoch_vl_loss.append(epoch_val_loss)\n  epoch_tr_acc.append(epoch_train_acc)\n  epoch_vl_acc.append(epoch_val_acc)\n  print(f'Epoch {epoch+1}') \n  print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n  print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n  if epoch_val_loss <= valid_loss_min:\n    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n    # torch.save(model.state_dict(), '../working/state_dict.pt')\n    valid_loss_min = epoch_val_loss\n  print(25*'==')\n\n#######################################################################################\n# plot the results from the training and validation accuracies\nfig = plt.figure(figsize = (20, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epoch_tr_acc, label='Train Acc')\nplt.plot(epoch_vl_acc, label='Validation Acc')\n# plt.ylim([70, 80])\nplt.title(\"Accuracy\")\nplt.legend()\nplt.grid()\n\nplt.subplot(1, 2, 2)\nplt.plot(epoch_tr_loss, label='Train loss')\nplt.plot(epoch_vl_loss, label='Validation loss')\nplt.title(\"Loss\")\nplt.legend()\nplt.grid()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:40:19.128572Z","iopub.execute_input":"2021-08-11T16:40:19.128915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# What's Next?\n\nYou can use this project template as a starting point to think about your own project. There are a lot of ways to continue, here we share with you some ideas you migth find useful:\n\n*   **Work on the Preproccesing.** We used a very rudimentary way to tokenize tweets. But there are better ways to preprocess the data. Can you think of a suitable way to preprocess the data for this particular task? How does the performance of the model change when the data is processed correctly?\n*   **Work on the Model.** The RNN model proposed in this notebook is not optimized at all. You can work on finding a better architecture or better hyperparamenters. May be using bidirectonal LSTMs or increasing the number of stacked layers can improve the performance, feel free to try different approaches.\n*   **Work on the Embedding.** Our model learnt an embedding during the training on this Twitter corpus for a particular task. You can explore the representation of different words in this learned embedding. Also, you can try using different word embeddings. You can train them on this corpus or you can use an embedding trained on another corpus of data. How does the change of the embedding affect the model performance?\n*   **Try sentiment analysis on another dataset.** There are lots of available dataset to work with, we can help you find one that is interesting to you. Do you belive that a sentiment analysis model trained on some corpus (Twitter dataset) will perform well on another type of data (for example, youtube comments)?\n\n","metadata":{"id":"1ZulB6qUIFGO"}}]}